#
## 形式语言与自动机
### 形式语言与自然语言的区别

**形式语言是字符串的数学集合，能够被精确识别，没有歧异，规则是严格定义的**
**而自然语言没有严格语法，语义在不同社会环境会不同，常具有多意性**
包含：  
-   字母表Σ：有限的符号集合
-   字符串：由字母表中符号组成的序列
-   Σ*：所有可能字符串的集合（包括空串）
### 形式语法(文法)
人为指定的一套严谨的语言规则，符合文法的语言输入到自动机中能够被自动机按照自动机的规则处理。
文法包含四元组：
-   1.非终结符集合N: 半成品，被产生式拆开
-   2.终结符集合Σ:最终结果，不被替换
-   3.产生式集合：把非终结符展开为其他符号串的规则
-   4.起始符号S:N中一个成员，输入

### 自动机
自动机是能够根据输入语句自动地根据预设规则执行简单或复杂操作地模型，能识别是否符合文法，符合则进行预设的操作，不符合则只执行对异常的终止处理。

-    1.有限自动机FA(DFA/NFA):只有有限的状态
-    2.下推自动机PDA：上下文无关的语言，加上一个栈
-    3.线性有限自动机LBA:
-    4.图灵机TM:任何可计算函数都能被图灵机计算

### 应用
-   **有限自动机**：分词、词性标注、拼写检查（DFA 实现字典匹配），Linux的grep文本搜索，电路设计
-   **下推**：句子语法分析，编译器语法分析(gcc处理if压栈 else弹栈)
-   **图灵机**：transformer的自回归，操作系统虚拟内存，cpython随机访问地址

## 语料库与语言模型
### 语料库
语料库是用于模型训练的大量自然语言文本的集合，处理为数学形式(矩阵)后，输入模型，供模型学习语言规律，如每个词的语义，上下文的关系，词语之间的关系。
### 语言模型
语言模型是能够学习自然语言规律并处理自然语言相关任务的模型，能够进行从简单的情感判断，到复杂的对话，解题等任务。
### n-gram
n-gram假设一个词的出现只与前N-1个词相关，每次预测相当于单标签多分类，N的设定为超参数。每次选取n个词，根据前面n-1个词，计算第n个词的条件概率分布，计算概率的方式有Unigram,biggram,Trigeram，选词算法有贪心算法，广度优先，束搜索。
**算法分类：**
-   贪心算法：每次选取分布式中概率最大的词为预测值，简单但视野短浅
-   广度优先：遍历每种可能词，一定能找到最优解但成本太大
-   束搜索：定义一个容量k，每一步根据概率和之前的分数选取分布中前k个词，再分别把这些束推进到下一词，得到k个分布列，对每个分布列中每个词打分，选取前k名继续束搜索；k=1时退化到贪心算法，k=分布式长度时变为广度优先

**概率计算**    

-   Unigram:P(A)=C(A)/C(All)基础，概率即单个词出现的频率
-   Biggram:P(M|N)=C(M,N)/C(N)，即N出现的状态下M同时出现的频率条件概率，上下文关联更强(N在M前)
-   Trigram:P(C|A,B)=C(C,A,B)/C(A,B)即AB同时出现的情况下C和AB也一起出现的频率(A,B在C前)

**n-grad特点和问题**：
-   训练速度快
-   可解释性强，比Transformer的键值查询易懂
-   能学习到基本语法和词义
-   十分依赖于训练的输入，若接收到训练时没有出现过的词，无法将其与训练语料的词联系起来，即使是近义词也无法识别其相关性。
-   受到窗口大小限制:
    -   窗口太小:只能根据有限词预测，上下文逻辑太短，
    -   窗口太大:[稀疏性] 即使不考虑计算量，由于语言本身具有低重复性，大语料中多数词都及其稀疏，概率小，泛化能力反而下降
    因此，n-gram无法处理长距离依赖和嵌套句式    

**平滑技术**
当语料较大时，多数词出现的频率将降低，对于本就出现得少的词，通过n-gram计算出来的概率会接近0。
为了解决稀疏性问题，平滑技术给低频词托底，有不同方式：
-   加一平滑/拉普拉斯平滑：对所有组合统一加上一个小量，包括那些计数为零的组合:P(A|B)=(C(AB)+1)/C(A+all)
-   基于回退的平滑：当高阶n-gram（如trigram）未出现或出现次数极少时，回退到低阶n-gram（如bigram或unigram）进行概率估计
-   将不同阶数的n-gram概率进行线性加权组合，而不是完全回退，既用低阶的Unigram保证了不会因组合低频而导致概率为0，也保留了高阶的信息
-   从高阶n-gram的计数中减去一个固定的小量(折扣)，将减去的概率质量重新分配给未出现的低阶事件,其最流行的变体是MKN，对不同计数值使用不同的折扣值，更精细地调整概率

## 词嵌入
### 词向量:
概念:把每个词都用一个高维向量表示.
意义:
-   表达丰富语义:每个维度都表示一种语义,语义相关的两个向量的空间距离近,两个向量的差可以表示他们在语义上的不同之处.
-   计算效率:多个词向量组成矩阵,能够并行运算,提高效率

### one-hot编码
-   one-hot(独热编码),将一类特征特征转化为由0和1构成的向量,长度是特征的种数,每种特征的独热编码只有一位是1,其余全是0,从而避免了用标量表示时隐含的数值大小和顺序    
-   但是词数量极多,若用one-hot编码会产生维度极高的向量(**维度灾难**),不利于计算,且其中只有一个维度是1,特征稀疏,并且每个向量都互相垂直,无法表达相关性

### Word3Vec
#### CBOW
给定一个窗口大小,每次用窗口内的上下文词预测中心词:

-    1.给上下文词one-hot编码 
-    2.通过嵌入矩阵将上下文one-hot编码a转化为n维向量 
-    3.把上下文向量输入线性层,最后经过softmax层得到中心词分别为什么词的概率,最后计算损失并优化   

## 大模型预训练和微调
### LLM两个阶段
预训练和微调
-   预训练：
    -   目的：让模型掌握通用语言能力和世界知识。通过海量数据的输入，模型广泛学习基本语言模式和上下文关系.
    -   方式:无监督学习，模型自己掩码再猜测，或判断两个句子是否相邻，计算预测概率的损失函数(交叉熵)，再反向传播优化各个参数
    -   效果：得到的多个参数：词嵌入时的词表，MLP的权重，偏置，层归一化的系数等，让微调阶段使用到良好参数
-   微调：
    -   目的：在预训练模型的基础上，根据不同任务增加结构，输入特定领域的语料，使模型能更好地完成特定任务。
    -   方式：监督学习，投喂特定领域的数据，在模型地头或尾增加一些层，优化全局或特定地参数
    -   效果：使模型在特定领域具有专业性，取下增加的层依然是原有模型
### 微调方法
-   全参数微调：继续更新所有参数，效果上限最高，但训练成本高，且不一定会更好
-   Lora:
    -   概述：不改动原有参数，而是外加参数矩阵▲W，使用时用W+▲W,微调时只更新外加的▲W，且切换任务时也只需要更换参数矩阵▲W
    -   计算：将▲W低秩分解为两个矩阵A@B；因为矩阵的信息往往分布不均匀，低秩矩阵可以用两个更小的矩阵表达再分别计算
    -   改进：
        -   Lura+:对A和B设置不同的学习率，靠近输出权重对梯度更敏感
        -   DoRa:将权重分解为方向V和幅度(标量)m,W=mV,V_=V+A@B,同时调整方向和幅度，减少LoRA仅用低秩可能丢失幅度信息
    -   特点：
        -   不改动影响原模型，避免遗忘原有能力
        -   速度快，内存占用小
        -   模块化：特定任务用不同的参数模块，而不用将原模型复制
-   Prompt：冻结预训练模型的全部参数，通过在输入层添加可训练的连续向量(软提示soft prompt)来引导模型完成特定任务,完全不改变模型参数

## 自回归 AI 和 生成式 AI
### 自回归模型
自回归模型是每一步根据已生成的词预测下一个词的文本生成模型。一段序列的联合概率是每个词的条件概率之积：```P(X_n,X_1,X_2...X_n-1)=P(X_n|X_n-1,X_n-2,X_n-3...X_1)P(X_n-1|X_n-2,X_n-3,...X_1)...P(X_2|X_1)P(X_1)```
### 逐词展开的原因
-   自然语言本身具有顺序，句子的生成应该从前到后
-   将生成一段话的任务分解为生成单个词，下一步可以利用这一步生成的信息，上下文的学习能力更强
-   计算公式的限制：联合概率的每一项P(X_t|X_t-1,X_t-2...X_1)是条件概率，计算出来的概率是**单个词**在已知上下文的条件下的概率

## Chain of Thought
### 实验题目：
设数列 {xₙ} 满足 x₀ = 1/3，xₙ₊₁ = xₙ²/(1 - xₙ + xₙ²)，n ≥ 0。证明无穷级数∑ₙ=0^∞ xₙ存在并计算结果
**答案：1/2**
### 实验过程：
分组：CoT&RAG,!CoT&RAG,CoT&!RAG,!CoT&!RAG
-   1.CoT&RAG:
    -   提示词：```[题目本身]+要求：CoT:分步推导，每过一个步骤务必检查正确性，再推进下一步```
    -   结果：1/2  正确
    -   LLM的输出过程：
        -   第一步：观察递推关系
        -   第二步：尝试计算前几项
        -   第三步：尝试寻找通项公式或递推关系的简化
        -   第四步：尝试寻找 yₙ 的通项或求和技巧
        -   第五步：尝试利用 yₙ 的表达式求和
        -   第六步：验证结果
-   2.!CoT&RAG：
    -   提示词：一开始，我的提示词是```[题目本身]+
请直接输出答案，不要写思考过程，不要分步，你只能输出答案本身，不能输出过程```
可是这个LLM可能在训练时就让它解题必须按照CoT分步求解，我再次多次提示“直接输出结果”无果，最终用CoT打败CoT,提示：```不要分步！再次强调，当你要输出时先思考自己的输出是否在展示过程，过程都不要，只能输出答案```
    -   结果：1错误
-   3.CoT&!RAG：
    -   提示词：```[题目本身]+要求：CoT:分步推导，每过一个步骤务必检查正确性，再推进下一步```
    -   输出结果：1/2  正确
    -   输出过程：
        -   第一步：理解题意
        -   第二步：分析递推关系
        -   第三步：寻找通项公式或简化形式
        -   第四步：考虑级数的收敛性
        -   第五步：估计xₙ的衰减速度
        -   第六步：计算级数的和
        -   第七步：验证结果
-   4.!CoT&!RAG
-   提示词：```[题目本身]
硬性要求：直接输出答案，不要分步！当你要即将输出时，先思考自己的输出是否在展示过程，请省略所有过程，只能输出答案```
这回汲取上次经验，我直接用用CoT让模型检查自己输出是否是过程，抑制模型本身要输出过程的的CoT
-   输出结果：1  错误

### 实验分析：
CoT有无的结果明显，CoT模式下模型分步骤解题，先算几项找规律或趋势，方法错误后尝试其他方法，最终会验证结果，与我们人类解题步骤极其相似。
然而，可能由于题目本身涉及的数学知识模型已经掌握，因此在CoT组中Rag不影响，我打算追加RAG的实验，询问具有时效性的问题

### 追加RAG实验
#### 对照组：
-   操作：关闭联网模式，让模型输出2025年诺贝尔化学奖得主
-   模型输出：```2025年诺贝尔化学奖于10月8日揭晓，获奖者为：
大卫·贝克（David Baker）（美国华盛顿大学蛋白质设计研究所）
约翰·江珀（John Jumper）（英国DeepMind公司）
德米斯·哈萨比斯（Demis Hassabis）（英国DeepMind公司```
-   分析：
**然而这是2024年的数据，说明模型上一次训练时还没有25年的语料**
**另外，模型自己说这是25年的，出现Ai幻觉，因此我用实验组1尝试用刚学的CoT，看看能不能消除幻觉**
#### 实验组1：
-   操作：关闭联网模式,增加CoT提示词```2025年诺贝尔化学奖得主，请你输出结果时确认是否是2025年化学诺奖得主```
-   模型输出：```很抱歉，我无法确认2025年诺贝尔化学奖得主。诺贝尔奖的评选过程高度保密，获奖信息通常在正式公布前不为公众所知。建议您关注官方渠道，如诺贝尔奖官网或权威新闻机构，以获取最新的诺贝尔奖信息。```
-   分析：体现了CoT能够有效减少Ai胡说八道的情况，至少‘不知为不知’
#### 实验组2：
-   操作：联网模式下问2025年诺贝尔化学奖得主
-   输出：```2025年诺贝尔化学奖的得主是：
Susumu Kitagawa（北川进）
Richard Robson（理查德·罗布森）
Omar M. Yaghi（奥马尔·M·亚吉）
他们因在金属-有机框架（MOFs）方面的开创性研究而获奖。这类材料具有高度多孔结构，可用于水收集、二氧化碳捕获、污染物去除、药物输送等多种应用```
-   分析：模型通过联网学习到新的信息，能够在不改变模型本身的情况下让LLM与时俱进

### 原理分析：
#### CoT:
CoT让模型运算时计算中间结果，反向传播时优化参数能针对其中某一步的运算参数。
#### RAG:
RAG不改变模型原本的参数，而是在词嵌入前在外部语料中寻找与输入文本最相关的文本，将文本拼接后再进行位置编码，词嵌入...如此一来，模型不用频繁拿最新语料训练，也能学习到实时的动态知识。

## LLM Mathematical Reasoning with Lean4
### Lean4
-   Lean4是一种用于写数学证明的函数式编程语言(属于形式语言)，逻辑严谨，形式化写定理，证明过程，能被计算机直接验证
-   选择原因：
    -   Lean4语法简洁，数学家易于使用
    -   是编程语言，能够运行来验证
    -   便于LLM理解数学概念，训练大模型
### 数据集
#### 数据生成过程
-   1.Informalization:用Gemini-1.5将每条Lean4生成自然语言的定理和证明过程
-   2.Example Retrieval:用微调的T5编码器检索最相似的NL(Natual Language自然语言)-FL(Formal Language形式语言)示例,提升Gemini的生成质量 
-   3.NL-FL Bootstrapping:把NL证明嵌入Lean4代码注释中,形成“注释式证明”,帮助模型理解推理过程
#### 克服自然语言推理与Lean4形式化推理之间的壁垒 的 方法
将Lean4翻译为形式语言后将其注释在每条Lean数学证明中，达到NL-FL(一条自然语言描述的数学定理和证明,能和一条Lean4形式化代码一一对应)
#### 训练过程
-   1.数据生成过程，如上
-   2.训练模型：
    -   1.按照证明步骤，由易到难将每个数学证明分层
    -   2.将k条相似的证明拼接，一起输入给模型，模型根据每一条地自然语言定理陈+NL证明+Lean4定理陈述来预测Lean4完整证明过程并加上自然语言注释，其中模型可以根据k条中已经预测过的样本学习证明的基本格式
    -   3.由易到难地给模型投喂单条样本，同上预测Lean4的完整证明过程
-   3.迭代式写作
每次从模型自己输出的预测中选取通过Lean4内核验证的样本作为示例，不断将外部示例替换为自己预测正确的证明，提高准确率   

<p style="color:yellow">其中除了推理阶段，用到LLM的地方：</p>
-   根据Lean4生成自然语言表述并插成注释；去重、截断、格式异常过滤 by Gemini-1.5 
-   检索最相似的FL和NL,by ByT5→T5
