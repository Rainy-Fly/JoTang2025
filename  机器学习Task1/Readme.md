# Task1
## MLP相关问题回答
### 1.多层感知机(MLP)
多层感知机是有多层神经元的神经网络,解决了传统单层网络无法进行异或运算的问题
*((0,1)与(1,0)输出1,(1,1)和(0,0)输出0,无法用线分开这两组点)*;包含输入层,隐藏层,输出层
-   输入层:样本特征数=神经元数,每个神经元将所有特征经过线性运算提取出一个新特征
-   隐藏层:重复h矩阵运算提取特征,激活函数,输出的过程
-   输出层:根据任务目标输出特定维度的向量或矩阵,得到预测值用于和标签比较
### 2.数据的作用
数据是样本的集合,每个样本具有多种特征,"喂"给机器,看他的参数和数据经过算法得到的结果和实际值的差异,从而不断优化,用数据的特征训练模型参数,用数据的标签评价和优化模型:具体来说:
-   噪声是样本中不能对预测目标起到帮助的信息,可以分为:1.无关变量,如文本处理时的标点符号 2.有误信息:调查取得的数据中可能有人未反映真实情况
-   特征是与预测目标有相关性,并能据此估计预测目标的有用信息,对于不同目的,不同的信息可能是或不是特征,相当于中学生物中同一个变量在不同情景下可能是自变量或无关变量
-   标签:模型最终根据特征要去拟合的值,是模型输出(因变量)的评判标准
-   batch_size是每次输入到MLP的样本数,样本被合并为[batch_size,feature_num]的矩阵,堆叠后可以利用矩阵并行运算的能力,减少访问内存的频率,优化性能
### 3.神经元
神经元是每一层的复合函数,每个神经元会提取由上一层每个神经元输出的特征(或输入层提取样本的feature_num个特征),内部有一个权重向量(一层所有神经元的权重叠成权重矩阵,每一列是一个神经元的权重矩阵)与输入特征加权平均,再加上偏置项得到结果,再经过一个激活函数输出
### 4.激活函数
激活函数是每层神经元最后的一个简单但重要的非线性函数.如果没有激活函数,再多层的MLP也只是多层线性函数的复合函数,还是线性函数,无法学习并预测非线性规律,只能进行标准的线性回归;而有了线性函数,每层都加一点非线性,经过多层后能够复合成任意非线性函数,学习复杂特征此即**非线性表达能力**
常见激活函数:
-   1.Relu:```f(x)=max(0,x)```  **优点**:缓解梯度消失.如果每层函数导数<1,最终梯度越来越小,而w=w-w.grad的grad极小,几乎不更新.而relu在正半轴的倒数n恒为1,梯度在他这里不会减小  **缺点**:x<0时n神经元输出为0,若这个神经元接收到的特征都<0,无论输入什么,他永远输出0,导致n神经元死亡
-   2.LeakyRelu:在Relun基础上让负数的输出为lambda*x,lambda是极小值,缓解神经元死亡
-   3.sigmoid:``f(x)=1/(1+e^(-x))``函数为S型曲线,**优点:**输出(0,1)的值,可表示概率,进行二分类  **缺点:**x->0 or 1时S曲线平缓,导数接近0,导致梯度消失,因此一般不在隐藏层用,而在输出层用
-   4.softmax:``记S(x)=每个e^x之和,则f(x)=e^x/S(x)``,a相当于先把每个输入取指数,对应输出是其对应指数在指数和的占比,处理十分类问题
-   5.tanh:没错,就是数学数上的双曲正切,```f(x)=[e^(x)-e^(-x)]/[e^(x)+e^(-x)]```,输出(-1,1),与sigmoid最大区别在与输出区间,根据实际情况选取
### 5.计算图
-   计算图是对前向传播的可视化,每个节点代表变量或函数操作,边代表数据流动的方向;其中没有父节点的点叫叶子,断点测试时可看到is_leaf=True/False,没有子节点的点叫根,MLP中是损失函数(加上正则化),反向传播时以根为自变量,叶子为因变量计算梯度;
-   数据结构和离散数学中的计算图没有方向,而MLP的计算图要表示前向传播的方向
<p style="color:yellow">我的make_moons神经网络的计算图:</p>

<img src="image.png" style="height:400 width:300">

### 6.参数
-   MLP的参数量是权重与偏置数量之和:每一层神经元有权重矩阵(输入特征数,神经元数)和偏置矩阵(神经元数,1),因此一层的参数量是:神经元数*(输入特征数+1),MLP的参数量就把每一层的参数量相加
-   超参数是反向传播过程中不进行优化的参数,只有在整个训练集训练完了后,才根据验证集指标调整.有:
    -   lr:learn rate 学习率,用来调整权重和偏置的调整幅度:w-=w.grad&lr
    -   batch_size:一次前向传播输入的样本量
    -   lambda:正则化大小,防止过拟合
    -   神经元的层数与每层的神经元数,即权重矩阵的数量和维度大小;
    -   epoch:训练轮数,每轮都把训练集跑完,可能跟着验证集衡量指标
### 7.隐藏层
-   隐藏层是除去输入层和输出层后,中间的神经层;因为这些层提取到的特征不是直接输入的特征,而是经过输入层提取到的特征;而它不直接输出预测结果,外部拿到的结果要经过输出层的神经元;因此,他既不与输入直接联系,也不直接输出,外部不可见,因此得名.
### 8.损失函数
-   损失函数计算预测值与实际值的差异,衡量误差,通过计算它的梯度,决定如何调整参数使损失值减小.<br>
常见损失函数:
-   1.均方误差:```-[sum((y_hat-y_label)**2)/len(y_hat)]```,即残差平方的均值,用于线性回归
-   2.交叉熵:```-[y_label*log(y_hat)+(1-y_hat)*(log(1-y_label))]``` 用于二分类或多分类
### 9.
-   前向传播:输入层 *提取特征* =>隐藏层 *不断提取高级特征*=>输出层 *输出预测结果*
-   梯度:自变量偏导数组成的向量,若自变量变化量组成的向量与梯度向量相反,则因变量减少最快
-   学习率:w=w-w.grad*lr的lr(learn rate),设置权重更新的幅度
-   反向传播:沿着计算图反向计算梯度,对权重矩阵和偏置矩阵减去他们的梯度与学习率之积,优化参数
-   优化器:
### 10.归一化,正则化
-   归一化是将一组数据处理到(0,1)的区间的方法,有两种方法:
    -   ```x=>(x-x.min())/(x.max()-x.min())```  但由于用最值压缩,容易受到极端值影响,比如最大值极大,分母太大,值接近0
    -   ```x=>(x-x.mean())/x.std()```用均值和方差计算,接近正态分布,缓解极值的影响
### 11.欠拟合,过拟合
-   欠拟合是模型没有学习到特征与结果的关系,无论训练集,验证集,测试集的损失函数都很大,"训练了个寂寞";     
    可能原因:
    -   神经元太少,隐藏层层数少,学习率太小,正则化过强,激活函数的非线性表达能力不够(折线relu不如曲线tanh)   
-   过拟合:过分地学习到样本的所有信息,为了拟合极端值而较大幅度地调整参数,以及将噪声用于预测,可以说是既学习了与目标有关的本质特征,又强行拟合杂乱信息.验证集的损失函数很小,但验证集和测试集不佳

## 代码实践 make_moons
### 激活函数的选择
最初,我选择Relu函数来拟合双月曲线,不断尝试调整样本量,学习率,神经元个数,和层数.但效果极其有限,模型出现过下列症状:
-   红色/蓝色某一方占绝对优势,模型几乎把所有样本都归为一类,0.5分界线在角落
-   分界线不平滑,而是锯齿形
-   分界处附近的预测值是0.5左右,但某一侧的梯度很大,略微移动鼠标,数值突变到1或1e-30;另一侧梯度小,即使在最外侧,数值也始终在0.4-0.5或0.5-0.6左右

如下图:

<img src="月亮热力图makemoon/make_moons_0(fail).png" style="height:200">
<img src="月亮热力图makemoon/moon_1.png" style="height:200">
 <img src="月亮热力图makemoon/moon4.png" style="height:200">

究其本源,relu只是两个简单的线性函数,仅仅两层无法学习到月形曲线的非线性特征,也就是非线性表达能力不强,又考虑到sigmoid容易梯度消失,我最终选择tanh激活函数,成功拟合:

<img src="月亮热力图makemoon/moon5.png" style="height:400">


