# Task4 NLP
## 一.传统模型

### RNN
-   选取batch_size个文本作为一个批次batch;把每段文本序列拆成处理的最小单位token,选取最长的token作为时间步长度time_steps,短的补0,对每个文本序列进行"一层"运算叫做其中一个时间步.
-   相较于传统MLP,每个时间步(n相当于n一层神经元)输出前先保留一个隐藏状态h_t,再经过激活函数才是输出y_t;每层输入多了上一层的隐藏状态h_t-1*权重W_h.如此一来,后面的token都能保留前文所有token的信息
```  python
h_t = f(W_h * h_t-1 + W_x * x_t + b)
y_t = g(W_y * h_t + c)
```
总的来说,RNN主要通过隐藏状态传递了上文信息,用于后文预测.但是有以下缺点:
-   1.序列长时时间步多,多次乘W_h,容易因为多层函数导致梯度爆炸(W_h>1)或消失(W_h<1);并且前期的token会被后期的token覆盖掉.导致RNN学的序列长度有限
-   2.每个时间步都依靠上一步的隐藏状态计算完成,只能串行运算,无法利用矩阵的并行运算,效率低

### GRU
为了解决RNN连乘权重矩阵导致的梯度爆炸或消失,GRU用重置门和更新门实现长距离依赖:
-   1.两个门的计算:都是将上一步的隐藏状态与这一步的输入在特征维度concat,再经过线性变换,最后经过激活函数sigmoid得到两个门(z重置门,r更新门)
-   2.使用重置门:将上一步的隐藏状态(h_t-1)与更新门逐元素相乘后,与这一步的输入concat,再线性变换得这一步的候选状态h~t
-   3,使用更新门:这一步的隐藏状态为h_t = (1 – z) ⊙ h_t-1 + z ⊙ h~t
GRU缓解了梯度问题,能够较长距离依赖;但依然是串行计算,距离不能太长.

## 二.Transformer
### 注意力机制
-   自注意力（Self-Attention）：输入序列内部的注意力机制，计算序列中每个元素与其他元素的相关性.
-   交叉注意力（Cross-Attention）：连接两个不同序列的注意力机制，如编码器-解码器模型中，解码器关注编码器的输出。
-   全局/局部注意力（Global/Local Attention）：全局注意力关注整个序列，局部注意力仅关注序列的局部窗口.
-   稀疏注意力（Sparse Attention）：通过限制注意力范围（如局部窗口、随机采样或固定模式）降低计算复杂度，如Longformer、BigBird.
-   分层注意力（Hierarchical Attention）：分层处理不同粒度的信息，如文档-句子-词的多层注意力.
-   多头注意力（Multi-Head Attention）：通过多个独立的注意力头捕捉不同子空间的特征，增强表达能力.   

常用的:多头自注意力(Transformer用的),交叉注意力,稀疏注意力
<img src="markdown的图片资源/截图 2025-10-04 10-07-54.png" style="height:400">

### 词向量:
概念:把每个词都用一个高维向量表示.
意义:
-   表达丰富语义:每个维度都表示一种语义,语义相关的两个向量的空间距离近,两个向量的差可以表示他们在语义上的不同之处.
-   计算效率:多个词向量组成矩阵,能够并行运算,提高效率

### one-hot编码
-   one-hot(独热编码),将一类特征特征转化为由0和1构成的向量,长度是特征的种数,每种特征的独热编码只有一位是1,其余全是0,从而避免了用标量表示时隐含的数值大小和顺序    
-   但是词数量极多,若用one-hot编码会产生维度极高的向量(**维度灾难**),不利于计算,且其中只有一个维度是1,特征稀疏,并且每个向量都互相垂直,无法表达相关性

### Word3Vec
#### CBOW
给定一个窗口大小,每次用窗口内的上下文词预测中心词:

-    1.给上下文词one-hot编码 
-    2.通过嵌入矩阵将上下文one-hot编码a转化为n维向量 
-    3.把上下文向量输入线性层,最后经过softmax层得到中心词分别为什么词的概率,最后计算损失并优化   

#### skip-gram
流程基本同CBOW,但是用中心词预测上下文词,对低频词的识别更好,训练速度较慢;而CBOW对高频词更敏感,训练速度更快

### 位置编码
-   本身为何没有位置信息:RNN中每个时间步都输入了上一步的隐藏状态,已经传递了位置信息,而transformer先对每个位置编码独立地进行词嵌入,注意力层的QKV与其他每个位置i进行相同运算,也不包含位置信息.如果每个token打乱顺序,QKVa计算的结果依旧一样
-   位置信息分类:
    -   不可学习的绝对位置编码:每个token是输入矩阵的一列,对于一列中每个元素,包含绝对位置信息:pos列索引(第pos个token),i行索引(第i维),通过公式:
<img src="markdown的图片资源/截图 2025-10-04 11-03-21.png" style="height:100">
    计算出输入矩阵中每个元素对应的绝对位置,将这个绝对位置的矩阵与嵌入后的矩阵相加,即是包含了绝对位置信息的矩阵.
        -   pros:简单轻量
        -   cons:三角函数的周期性会让远距离文本的位置编码不好区分,不善于处理长文本
    -   可学习的绝对位置编码:input Embedding的嵌入矩阵不再是固定的,而是可学习的,Positional Encoding使用由这个可学习的嵌入矩阵计算出的矩阵计算位置信息
        -   pros:对于给定长度的文本效果又快又好
        -   cons:序列长度增加就会使性能迅速下降
    -   可学习的相对位置编码:在进行查询Q*K计算时,n加上i偏置项b(i,j),b根据查询和被查询token的位置在相对位置偏置矩阵中索引得到,这个矩阵是可学习的.
        -   pros:长文本处理能力强,能捕获相对位置
        -   cons:繁琐
-   LLM常用位置编码
    -   1.(**RoPE**)旋转位置编码:对 Q/K 的每个维度，按 Token 的绝对位置pos进行极坐标旋转:超长文本处理和外推(处理比训练时更长的文本)好,细粒度(位置接近的token)区分好,较复杂
    -   2.(**ALiBi**)注意力线性偏置:对于原始的S=Q@K加上偏置B,任意两个token的相对距离nn乘上偏置.系数m就是偏置的某一项:无额外参数,能应对超长文本与外推情况,但细粒度区分稍弱
    -   3.(**Learned Absolute**)可学习绝对位置:简单,长度固定

### 层归一化 LayerNorm
-   Layer Normalizationy应用于多头注意力子层或前馈网络子层的输出张量(batch, seq_len, d_model),分别对其中一个seq中一个token的所有特征归一化,使得特征接近正态分布,使不同特征的尺度相同,训练稳定
-    BatchNormalization对每个特征的所有样本和token的平面归一化,小批次时均值和方差的估计不稳定

   
### mask
 Causal Mask / Look-ahead Mask是一个上三角矩阵,对角线上为负无穷,把掩码加到注意力分数scores=Q@K/sqrt(d_k)上:因为注意力分数的行是查询的token,列是被查询的token,上三角的位置代表被查询的token在查询的token之后,加上负无穷后,经过sigmoid,其概率接近0,从而让解码器看不到将来的信息,只能根据已有信息预测最新的位置

### 束搜索
LLM的解码器最终输出当前预测的token填词汇表中每个词的概率,整个概率向量的长度是词库的大小,是一个分布列.束搜索在分布列中选取概率前k大的词,分成k条束去预测再下一个词,每条束遇到以下两种情况就会终止:  
-   1.某束预测出的词是词表中的\<EOS>
-   2.某束长度超过设定的最大长度
当所有束停止,就选取总概率最大的为最终预测词


## 三.BERT
 
### 基本架构和原理
#### 粗略框架:BERT取了transformer中的编码器Encoder部分,用更多的注意力头和隐藏层数,叠加多个Encoder块,把输入的大批量语料进行自监督学习,为后续不同类别的任务提供预训练.
<img src="markdown的图片资源/Screenshot 2025-10-04 at 19-23-31 bert论文 - 搜索 图片.png" style="height:300">

#### 流程:
-   分词预处理:将一段序列按照指定方式分成多个token(汉字/单词/前缀后缀),再在每段话的开头和句间加特殊符号;然后根据预设最大长度,对长的序列截断超出的,给短的序列补[PAD]
-   词嵌入:根据固定的词表把每个token映射为标量数字编号,再用编号去可学习的嵌入矩阵索引,每个token对应一个向量.
-   位置嵌入:根据可学习的矩阵,每个token根据序列中的位置都对应一个向量,加到原向量中.
-   段嵌入:每个句子在段中的位置对应一个向量,加到句中每个token的向量上
-   LayerNorm(标准化后再线性计算)+Dropout(随机把一些值设为0)
-   多头:每个token分别乘三个相同矩阵得到他的Q(Query查询),K(Key键),V(Value值);每个token都用他的Q与每个K(包括他自己)计算:```softmax(Q@K/sqrt(d_k向量长度))```得到他关于每个token的关系程度,即一组权重,去和每个token的V相乘,得到纳入了他和其他每个向量的关系以及位置信息的向量,至此,每个token的向量都拥有了丰富的上下文信息
-   (Add&Norm)残差连接+LayerNorm:y=LayerNorm(x + F(x)),多了把原向量也输入的操作,使得当某一层效果不好时可以保留信息,不至于更糟(早期人们发现神经网络层数增加到一定层之后再增加的话效果不升反降,于是采用残差连接)
-   (Feed Forward)前馈网络:FFN(x) = max(0, x W1 + b1) W2 + b2,增加非线性表达能力
-   (Add&Norm)again 

多层上述步骤后,预训练预测掩码[MASK]和判断句间关系,计算损失,接反向传播;微调根据下游任务来设置,比如情感二分类就取最后一个token的结果向量送进经典MLP**不用像卷积一样展平,因为每个token都包含了各个位置的信息,况且BERT没有卷积的池化,展开后太长了**

### 双向原理
对输入Encoder的序列随机选取15%的token处理:其中80%用[Mask]替换,剩下各10%随机用其他词替换以及不变.如此一来,[Mask]分布在句子中间,注意力头识别整句各输入对[Mask]预测,如同我们中学的完形填空,根据上下文预测[Mask]

### 预训练任务
-   通过Masked Language Model(MLM)学习词义,搭配,能根据上下文分析含义
-   通过Next Sentence Prediction(NSP)学习到句间关系
-   结果:学习到:嵌入矩阵,注意力头,LayerNorm的可学习系数
训练得到嵌入矩阵的查表,用于微调时将分词并特殊标记后的序列的meigtoken与对应这个训练好的表一一对应,编号.

### 输入嵌入(Input Embedding)
-   词嵌入(Token Embedding):把自然语言的词转换为向量.**能够包含多个维度的语义特征,还能并行计算**
-   段嵌入(Segment Embedding):每段段首添加标识符\<CLS>,并且把50%正确顺序的句子合在一起,中间用\<Sep>分隔,50%错误顺序的句子同理操作.**让模型不仅把我词在上下文的语义,还能理解句间的关系**
-   位置嵌入(Position Embedding):在每句中把token的句内位置映射到一个可学习的表中,得到每个token的位置嵌入.**BERT本身就是transformer演化来的,其自注意力机制本身不含位置信息,需要额外添加**

### 和Transformer的区别
-   输入: 分为词嵌入,段嵌入,位置嵌入;段嵌入捕获了transformer输入没有的句间关系,位置嵌入不是三角函数,而是根据索引映射到可学习的矩阵
-   编码器部分:层数和注意力头更多
-   输出:不接解码器,而是接处理词义的MLM-head或处理句间关系的NSP(普通MLP),计算损失,反向传播,进行自监督学习
-   结果:优化查表的参数和解码器中注意力头,前馈神经网络,层归一化的参数,用于下游任务;而transformer的解码器只是把序列转化为携带语义和位置关系的矩阵,传给编码器作为输入之一,用于预测   


